{
  "paragraphs": [
    {
      "text": "%md\n# Projet GDELT - Script ETL - janvier 2021\n#### Auteur : Chloé RICHARD, Céline DREVET, Enzo MARTI, Maxime GEAY\n\n### *Partie 1 : Chargement des données dans S3*",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:27:51+0100",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Projet GDELT - Script ETL - janvier 2021</h1>\n<h4>Auteur : Chloé RICHARD, Céline DREVET, Enzo MARTI, Maxime GEAY</h4>\n<h3><em>Partie 1 : Chargement des données dans S3</em></h3>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210204_1907753303",
      "id": "20200112-094551_1494650104",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2706",
      "dateFinished": "2021-01-24T23:27:52+0100",
      "dateStarted": "2021-01-24T23:27:51+0100"
    },
    {
      "text": "%md\n#### Librairies utilisées",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h4>Librairies utilisées</h4>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210205_100128944",
      "id": "20200112-100417_1739530681",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2707"
    },
    {
      "text": "import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection \nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\n//import com.datastax.spark.connector.cql.CassandraConnector\n//import org.apache.spark.sql.cassandra._\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210205_254347191",
      "id": "20200112-100444_1627983331",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2708"
    },
    {
      "text": "%md\n#### Etape 1 : extraction des données et stockage dans un bucket S3\nPour cela, on télécharge dans un premier temps un fichier CSV *masterfilelist.txt* qui  contient la liste des fichiers du jeu de données GDELT ainsi que l'URL pour télécharger chaque fichier. Chaque fichier de données est accessible par HTTP. La fonction *fileDownloader* permet de télécharger un fichier. \nCe fichier est ensuite convertit en Data Frame, filtré sur la journée qui nous intéresse, puis grâce aux URL, on récupère les données GDELT et on les convertit en Data Frame également :\n\n- les events \n- le graphe des relations \n",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h4>Etape 1 : extraction des données et stockage dans un bucket S3</h4>\n<p>Pour cela, on télécharge dans un premier temps un fichier CSV <em>masterfilelist.txt</em> qui contient la liste des fichiers du jeu de données GDELT ainsi que l&rsquo;URL pour télécharger chaque fichier. Chaque fichier de données est accessible par HTTP. La fonction <em>fileDownloader</em> permet de télécharger un fichier.<br/>Ce fichier est ensuite convertit en Data Frame, filtré sur la journée qui nous intéresse, puis grâce aux URL, on récupère les données GDELT et on les convertit en Data Frame également :</p>\n<ul>\n  <li>les events</li>\n  <li>le graphe des relations</li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210205_1487251410",
      "id": "20200112-094616_141408279",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2709"
    },
    {
      "text": "// Configuration de l'accès au bucket S3\n    \n// == Code avec identifiants AWS ==\n\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicSessionCredentials\n//import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n\nval AWS_ID = \"ASIAQKINZEMYEANJJXPB\"\nval AWS_KEY = \"NDSRMBGu7DZ7ENCflsfjhctM0bCbJMeZ+eEXRfLZ\"\nval AWS_TOKEN = \"FwoGZXIvYXdzELb//////////wEaDH2N9VgCp3KhCFaYHSLOAT3O1KhpG64FU1VvKK6ozt7CyS9B+eJ//qxh04sSjeeRyws2mUNg3pp2lg1UbnIV9g1m5hEE3iu26fEd3La0yux5YHoEjP08op8Ome7i4qar/GY/iHmW5m40kjJoeJZABdMhqmtgH0pnHjVmq/oIwRyUMyEij8JVlDquE9exjp4mRrIiZ//FnlD090ZJo5U346kerFUG5cssIg3UJwNQTy73PDcbVmIAoQFhw7IcRZtNlSinhGo0iYBX7L8Zl5qsE/gSegiIlhYoTU+RNGHCKKSJnYAGMi20p9qlJAacZRetftX0DMGK9otNAbdv/zlnj77lltRC5rJ90pAScurOZbgXjqM=\"\n// la classe AmazonS3Client n'est pas serializable\n// on rajoute l'annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l'envoyer aux executeurs\n@transient val awsClient = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_TOKEN))\n\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.session.token\", AWS_KEY) // mettre votre token dans Account Details\nsc.hadoopConfiguration.setInt(\"fs.s3a.connection.maximum\", 5000) // augmenter la durée de connexion à une session aws\n\n// == Code sans identifiants AWS ==\n\n//import com.amazonaws.services.s3.AmazonS3ClientBuilder\n//import com.amazonaws.services.s3.AmazonS3\n\n//@transient val awsClient = AmazonS3ClientBuilder.standard().withRegion(\"us-east-1\").build();\n//awsClient.putObject(\"thomas-gdelt\", \"masterfilelist.txt\", new File( \"/home/ubuntu/data/masterfilelist.txt\"))\n//awsClient.putObject(\"thomas-gdelt\", \"masterfilelist-translation.txt\", new File( \"/home/ubuntu/data/masterfilelist-translation.txt\"))\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "warning: there was one deprecation warning; re-run with -deprecation for details\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicSessionCredentials\nAWS_ID: String = ASIAQKINZEMYBA6NBVUC\nAWS_KEY: String = 7glyl3zPNxKjgsQwIFzhn6BVgbmkCvRObwnhb4WJ\nAWS_TOKEN: String = FwoGZXIvYXdzELT//////////wEaDJplLdYnWBkg8YB5ZiLOAWtXTtxrnJETxvf/fW3TeLX9YLgD1+/rwVlpGSpZq5l6K9SKNbMbbfCePGkYdAhqdu2E2soOZt5S5XI2l1VDmEnUNtOm4YTpIRxquv2k3JGMFFqYOClo2+OcLhOrphrbL4MI6GeaTQ8QgDDud/u+RkKLQ6kBP+yr9SeK22QnOwFUV9VxlEID5dzTXv3ENxnxGltuPWduZ1FFUbVdreXYZk758w9eb9e7+jD/CHAiDPptUQSauwsQDueEpOgtPovO3HwRk143vXXRo4482gy6KLLRnIAGMi3mepYvQ1S1NasnvG7bF8tEt5OIDRHOsUUeGs500wQM464mblQvYYm0G9UT8kY=\nawsClient: com.amazonaws.services.s3.AmazonS3Client = com.amazonaws.services.s3.AmazonS3Client@629e96b9\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210206_2070957601",
      "id": "20200112-102836_1663931357",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2710"
    },
    {
      "text": "// Fonction fileDownloader\ndef fileDownloader(urlOfFileToDownload: String, fileName: String) = {\n    val url = new URL(urlOfFileToDownload)\n    val connection = url.openConnection().asInstanceOf[HttpURLConnection]\n    connection.setConnectTimeout(5000)\n    connection.setReadTimeout(5000)\n    connection.connect()\n\n    if (connection.getResponseCode >= 400)\n        println(\"error\")\n    else\n        url #> new File(fileName) !!\n}",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "warning: there was one feature warning; re-run with -feature for details\nfileDownloader: (urlOfFileToDownload: String, fileName: String)Any\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210206_724872097",
      "id": "20200112-100354_934449719",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2711"
    },
    {
      "text": "// Téléchargement du fichier masterfilelist.txt et sauvegarde dans le dossier /tmp/ du bucket gdelt-chloe-bucket\nfileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\", \"/tmp/masterfilelist.txt\")\nfileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt\", \"/tmp/masterfilelist_translation.txt\") //same for Translation file\nawsClient.putObject(\"gdelt-chloe-bucket\", \"masterfilelist.txt\", new File(\"/tmp/masterfilelist.txt\") )\nawsClient.putObject(\"gdelt-chloe-bucket\", \"masterfilelist_translation.txt\", new File( \"/tmp/masterfilelist_translation.txt\") )",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res2: com.amazonaws.services.s3.model.PutObjectResult = com.amazonaws.services.s3.model.PutObjectResult@445ef043\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210207_898435526",
      "id": "20200112-100557_1341750775",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2712"
    },
    {
      "text": "object AwsClient{\n    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_TOKEN))\n}",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "warning: there was one deprecation warning; re-run with -deprecation for details\ndefined object AwsClient\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210207_1403947294",
      "id": "20200120-110146_602857232",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2713"
    },
    {
      "text": "// Conversion du fichier masterfilelist.txt en Data Frame\nval sqlContext = new SQLContext(sc)\nval files_english_DF = sqlContext.read.\n                    option(\"delimiter\",\" \").\n                    option(\"infer_schema\",\"true\").\n                    csv(\"s3://gdelt-chloe-bucket/masterfilelist.txt\").\n                    withColumnRenamed(\"_c0\",\"size\").\n                    withColumnRenamed(\"_c1\",\"hash\").\n                    withColumnRenamed(\"_c2\",\"url\").\n                    cache\n// Filtre sur une année de données \nval englishDF = files_english_DF.filter(col(\"url\").contains(\"/2020\")).cache\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "warning: there was one deprecation warning; re-run with -deprecation for details\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@7cd683bf\nfiles_english_DF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\nenglishDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210207_557312633",
      "id": "20200112-101505_1386341713",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2714"
    },
    {
      "text": "%md \nUn fichier empêche l'import de toutes les données sur S3 car il est introuvable. On le supprime donc de la liste des fichiers csv afin de permettre l'extraction et l'import des fichiers sur S3.",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Un fichier empêche l&rsquo;import de toutes les données sur S3 car il est introuvable. On le supprime donc de la liste des fichiers csv afin de permettre l&rsquo;extraction et l&rsquo;import des fichiers sur S3.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210207_1170348440",
      "id": "20210119-150938_330869936",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2715"
    },
    {
      "text": "val englishDF_ = englishDF.filter(!col(\"url\").contains(\"20201214121500\"))",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "englishDF_: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210208_716292133",
      "id": "20210119-145347_178792777",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2716"
    },
    {
      "text": "// check de la suppression de la ligne \nenglishDF_.filter(col(\"url\").contains(\"20201214121500\")).show\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----+----+---+\n|size|hash|url|\n+----+----+---+\n+----+----+---+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210208_1794134797",
      "id": "20210119-150744_1185952828",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2717"
    },
    {
      "text": "// Chargement des fichiers de données dans le bucket S3\nenglishDF_.select(\"url\").repartition(100).foreach( r=> {\n            val URL = r.getAs[String](0)\n            val fileName = r.getAs[String](0).split(\"/\").last\n            val dir = \"/tmp/\"\n            val localFileName = dir + fileName\n            fileDownloader(URL,  localFileName)\n            val localFile = new File(localFileName)\n            AwsClient.s3.putObject(\"gdelt-chloe-bucket/gdelt-csv-files\", fileName, localFile )\n            localFile.delete()\n})",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210208_548798051",
      "id": "20210119-145340_872312660",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2718"
    },
    {
      "text": "// Conversion du fichier masterfilelist_translation.txt en Data Frame\nval sqlContext = new SQLContext(sc)\nval files_others_DF = sqlContext.read.\n                    option(\"delimiter\",\" \").\n                    option(\"infer_schema\",\"true\").\n                    csv(\"s3://gdelt-chloe-bucket/masterfilelist_translation.txt\").\n                    withColumnRenamed(\"_c0\",\"size\").\n                    withColumnRenamed(\"_c1\",\"hash\").\n                    withColumnRenamed(\"_c2\",\"url\").\n                    cache\n// Filtre sur une année de données \nval othersDF = files_others_DF.filter(col(\"url\").contains(\"/2020\")).cache\n// Chargement des fichiers de données dans le bucket S3\nothersDF.select(\"url\").repartition(100).foreach( r=> {\n            val URL = r.getAs[String](0)\n            val fileName = r.getAs[String](0).split(\"/\").last\n            val dir = \"/tmp/\"\n            val localFileName = dir + fileName\n            fileDownloader(URL,  localFileName)\n            val localFile = new File(localFileName)\n            AwsClient.s3.putObject(\"gdelt-chloe-bucket/gdelt-csv-files\", fileName, localFile )\n            localFile.delete()\n            \n})",
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "warning: there was one deprecation warning; re-run with -deprecation for details\norg.apache.spark.SparkException: Job 92 cancelled part of cancelled job group zeppelin|anonymous|2FY1SESNS|20200120-110220_1900400504\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2080)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2015)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:968)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:968)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:968)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:968)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2270)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2250)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2239)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:799)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:972)\n  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:970)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.foreach(RDD.scala:970)\n  at org.apache.spark.sql.Dataset$$anonfun$foreach$1.apply$mcV$sp(Dataset.scala:2725)\n  at org.apache.spark.sql.Dataset$$anonfun$foreach$1.apply(Dataset.scala:2725)\n  at org.apache.spark.sql.Dataset$$anonfun$foreach$1.apply(Dataset.scala:2725)\n  at org.apache.spark.sql.Dataset$$anonfun$withNewRDDExecutionId$1.apply(Dataset.scala:3378)\n  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n  at org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n  at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3376)\n  at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2724)\n  ... 81 elided\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210208_461424969",
      "id": "20200120-110220_1900400504",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2719"
    },
    {
      "user": "anonymous",
      "dateUpdated": "2021-01-24T23:26:50+0100",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611527210209_1452818268",
      "id": "20210119-155752_777040543",
      "dateCreated": "2021-01-24T23:26:50+0100",
      "status": "READY",
      "$$hashKey": "object:2720"
    }
  ],
  "name": "Partie-1_Stockage_donnees_S3",
  "id": "2FVAJKVZJ",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Partie-1_Stockage_donnees_S3"
}