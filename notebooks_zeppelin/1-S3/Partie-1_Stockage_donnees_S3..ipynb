{
  "metadata": {
    "name": "Partie-1_Stockage_donnees_S3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Projet GDELT - Script ETL - janvier 2021\n#### Auteur : Chloé RICHARD, Céline DREVET, Enzo MARTI, Maxime GEAY\n\n### *Partie 1 : Chargement des données dans S3*"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Librairies utilisées"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection \nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\n//import com.datastax.spark.connector.cql.CassandraConnector\n//import org.apache.spark.sql.cassandra._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Etape 1 : extraction des données et stockage dans un bucket S3\nPour cela, on télécharge dans un premier temps un fichier CSV *masterfilelist.txt* qui  contient la liste des fichiers du jeu de données GDELT ainsi que l\u0027URL pour télécharger chaque fichier. Chaque fichier de données est accessible par HTTP. La fonction *fileDownloader* permet de télécharger un fichier. \nCe fichier est ensuite convertit en Data Frame, filtré sur la journée qui nous intéresse, puis grâce aux URL, on récupère les données GDELT et on les convertit en Data Frame également :\n\n- les events \n- le graphe des relations \n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Configuration de l\u0027accès au bucket S3\n    \n// \u003d\u003d Code avec identifiants AWS \u003d\u003d\n\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicSessionCredentials\n//import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n\nval AWS_ID \u003d \"ASIAQKINZEMYEANJJXPB\"\nval AWS_KEY \u003d \"NDSRMBGu7DZ7ENCflsfjhctM0bCbJMeZ+eEXRfLZ\"\nval AWS_TOKEN \u003d \"FwoGZXIvYXdzELb//////////wEaDH2N9VgCp3KhCFaYHSLOAT3O1KhpG64FU1VvKK6ozt7CyS9B+eJ//qxh04sSjeeRyws2mUNg3pp2lg1UbnIV9g1m5hEE3iu26fEd3La0yux5YHoEjP08op8Ome7i4qar/GY/iHmW5m40kjJoeJZABdMhqmtgH0pnHjVmq/oIwRyUMyEij8JVlDquE9exjp4mRrIiZ//FnlD090ZJo5U346kerFUG5cssIg3UJwNQTy73PDcbVmIAoQFhw7IcRZtNlSinhGo0iYBX7L8Zl5qsE/gSegiIlhYoTU+RNGHCKKSJnYAGMi20p9qlJAacZRetftX0DMGK9otNAbdv/zlnj77lltRC5rJ90pAScurOZbgXjqM\u003d\"\n// la classe AmazonS3Client n\u0027est pas serializable\n// on rajoute l\u0027annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l\u0027envoyer aux executeurs\n@transient val awsClient \u003d new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_TOKEN))\n\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.session.token\", AWS_KEY) // mettre votre token dans Account Details\nsc.hadoopConfiguration.setInt(\"fs.s3a.connection.maximum\", 5000) // augmenter la durée de connexion à une session aws\n\n// \u003d\u003d Code sans identifiants AWS \u003d\u003d\n\n//import com.amazonaws.services.s3.AmazonS3ClientBuilder\n//import com.amazonaws.services.s3.AmazonS3\n\n//@transient val awsClient \u003d AmazonS3ClientBuilder.standard().withRegion(\"us-east-1\").build();\n//awsClient.putObject(\"thomas-gdelt\", \"masterfilelist.txt\", new File( \"/home/ubuntu/data/masterfilelist.txt\"))\n//awsClient.putObject(\"thomas-gdelt\", \"masterfilelist-translation.txt\", new File( \"/home/ubuntu/data/masterfilelist-translation.txt\"))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Fonction fileDownloader\ndef fileDownloader(urlOfFileToDownload: String, fileName: String) \u003d {\n    val url \u003d new URL(urlOfFileToDownload)\n    val connection \u003d url.openConnection().asInstanceOf[HttpURLConnection]\n    connection.setConnectTimeout(5000)\n    connection.setReadTimeout(5000)\n    connection.connect()\n\n    if (connection.getResponseCode \u003e\u003d 400)\n        println(\"error\")\n    else\n        url #\u003e new File(fileName) !!\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Téléchargement du fichier masterfilelist.txt et sauvegarde dans le dossier /tmp/ du bucket gdelt-chloe-bucket\nfileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\", \"/tmp/masterfilelist.txt\")\nfileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt\", \"/tmp/masterfilelist_translation.txt\") //same for Translation file\nawsClient.putObject(\"gdelt-chloe-bucket\", \"masterfilelist.txt\", new File(\"/tmp/masterfilelist.txt\") )\nawsClient.putObject(\"gdelt-chloe-bucket\", \"masterfilelist_translation.txt\", new File( \"/tmp/masterfilelist_translation.txt\") )"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "object AwsClient{\n    val s3 \u003d new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_TOKEN))\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Conversion du fichier masterfilelist.txt en Data Frame\nval sqlContext \u003d new SQLContext(sc)\nval files_english_DF \u003d sqlContext.read.\n                    option(\"delimiter\",\" \").\n                    option(\"infer_schema\",\"true\").\n                    csv(\"s3://gdelt-chloe-bucket/masterfilelist.txt\").\n                    withColumnRenamed(\"_c0\",\"size\").\n                    withColumnRenamed(\"_c1\",\"hash\").\n                    withColumnRenamed(\"_c2\",\"url\").\n                    cache\n// Filtre sur une année de données \nval englishDF \u003d files_english_DF.filter(col(\"url\").contains(\"/2020\")).cache\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \nUn fichier empêche l\u0027import de toutes les données sur S3 car il est introuvable. On le supprime donc de la liste des fichiers csv afin de permettre l\u0027extraction et l\u0027import des fichiers sur S3."
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val englishDF_ \u003d englishDF.filter(!col(\"url\").contains(\"20201214121500\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// check de la suppression de la ligne \nenglishDF_.filter(col(\"url\").contains(\"20201214121500\")).show\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Chargement des fichiers de données dans le bucket S3\nenglishDF_.select(\"url\").repartition(100).foreach( r\u003d\u003e {\n            val URL \u003d r.getAs[String](0)\n            val fileName \u003d r.getAs[String](0).split(\"/\").last\n            val dir \u003d \"/tmp/\"\n            val localFileName \u003d dir + fileName\n            fileDownloader(URL,  localFileName)\n            val localFile \u003d new File(localFileName)\n            AwsClient.s3.putObject(\"gdelt-chloe-bucket/gdelt-csv-files\", fileName, localFile )\n            localFile.delete()\n})"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Conversion du fichier masterfilelist_translation.txt en Data Frame\nval sqlContext \u003d new SQLContext(sc)\nval files_others_DF \u003d sqlContext.read.\n                    option(\"delimiter\",\" \").\n                    option(\"infer_schema\",\"true\").\n                    csv(\"s3://gdelt-chloe-bucket/masterfilelist_translation.txt\").\n                    withColumnRenamed(\"_c0\",\"size\").\n                    withColumnRenamed(\"_c1\",\"hash\").\n                    withColumnRenamed(\"_c2\",\"url\").\n                    cache\n// Filtre sur une année de données \nval othersDF \u003d files_others_DF.filter(col(\"url\").contains(\"/2020\")).cache\n// Chargement des fichiers de données dans le bucket S3\nothersDF.select(\"url\").repartition(100).foreach( r\u003d\u003e {\n            val URL \u003d r.getAs[String](0)\n            val fileName \u003d r.getAs[String](0).split(\"/\").last\n            val dir \u003d \"/tmp/\"\n            val localFileName \u003d dir + fileName\n            fileDownloader(URL,  localFileName)\n            val localFile \u003d new File(localFileName)\n            AwsClient.s3.putObject(\"gdelt-chloe-bucket/gdelt-csv-files\", fileName, localFile )\n            localFile.delete()\n            \n})"
    }
  ]
}